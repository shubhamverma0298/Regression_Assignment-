{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Regression_Assignment**"
      ],
      "metadata": {
        "id": "jqLi8DDyjfm1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q 1 What is Simple Linear Regression?**\n",
        "  - Simple linear regression is used to estimate the relationship between two quantitative variables.\n",
        "  - Equation = hthitha(x)=thitha(0) +thitha(x)\n",
        "  - One dependent variable and one independent variable\n",
        "\n",
        "**Q 2 What are the key assumptions of Simple Linear Regression?**\n",
        "  - The key assumptions of simple linear regression are :-\n",
        "  - The relationship between the independent and dependent variables is linear.\n",
        "  - Errors have constant variance (homoscedasticity).\n",
        "  - Errors are independent.\n",
        "  - Errors are normally distributed.\n",
        "\n",
        "**Q 3 What does the coefficient m represent in the equation Y=mX+c?**\n",
        "  - line y = mx + c,\n",
        "  - M is the slope of the line\n",
        "  - C is the y-intercept of the line. T\n",
        "  - This line cuts the y-axis at the point (0, c) which is at a distance of c units from the origin.\n",
        "\n",
        "**Q 4 What does the intercept c represent in the equation Y=mX+c ?**\n",
        "  - line y = mx + c,\n",
        "  - M is the slope of the line\n",
        "  - C is the y-intercept of the line. T\n",
        "  - This line cuts the y-axis at the point (0, c) which is at a distance of c units from the origin.\n",
        "\n",
        "**Q 5 How do we calculate the slope m in Simple Linear Regression?**\n",
        "  - In simple linear regression :-  \n",
        "   the slope m is calculated using the formula: m = r * (sy / sx), where r is the correlation coefficient, sy is the standard deviation of the dependent variable, and sx is the standard deviation of the independent variable.\n",
        "\n",
        "**Q 6 What is the purpose of the least squares method in Simple Linear Regression?**\n",
        "  -The least squares method in simple linear regression is used to find the line of best fit that minimizes the sum of the squared differences between the observed data points and the predicted values on the regression line.\n",
        "\n",
        "**Q 7  How is the coefficient of determination (R²) interpreted in Simple Linear Regression ?**\n",
        "  - In simple linear regression, the coefficient of determination (R²) indicates the proportion of the total variation in the dependent variable that is explained by the independent variable.\n",
        "\n",
        "**Q 8  What is Multiple Linear Regression ?**\n",
        "  - Multiple linear regression is a regression model that estimates the relationship between a quantitative dependent variable and two or more independent variables using a straight line.\n",
        "\n",
        "**Q 9 What is the main difference between Simple and Multiple Linear Regression?**\n",
        "  - The primary difference between simple and multiple linear regression lies in the number of independent variables used to predict a dependent variable. Simple linear regression uses one independent variable, while multiple linear regression uses two or more.\n",
        "\n",
        "**Q 10 What are the key assumptions of Multiple Linear Regression?**\n",
        "  - The key assumptions of multiple linear regression include\n",
        "     linearity, independence, homoscedasticity, and normality\n",
        "\n",
        "**Q 11 What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?**\n",
        "  -Heteroscedasticity in a multiple linear regression model refers to the non-constant variance of the error terms (residuals) across different values of the independent variables\n",
        "\n",
        "**Q 12 How can you improve a Multiple Linear Regression model with high multicollinearity?**\n",
        "  - To address multicollinearity in a multiple linear regression model, you can consider several strategies, including removing highly correlated variables, combining them, or using techniques like Principal Component Analysis (PCA) to reduce dimensionality and mitigate the issue according to Analytics Vidhya\n",
        "\n",
        "**Q 13 What are some common techniques for transforming categorical variables for use in regression models?**\n",
        "  - Several techniques transform categorical variables for use in regression models, including dummy coding (or one-hot encoding), label encoding, and target encoding\n",
        "\n",
        "**Q 14 What is the role of interaction terms in Multiple Linear Regression?**\n",
        "  - In Multiple Linear Regression, interaction terms represent situations where the effect of one independent variable on the dependent variable changes depending on the value of another independent variable\n",
        "\n",
        "**Q 15 How can the interpretation of intercept differ between Simple and Multiple Linear Regression?**\n",
        "  - In both simple and multiple linear regression, the intercept represents the expected value of the dependent variable when all independent variables are zero. However, the interpretation and importance of the intercept differ slightly between the two models\n",
        "  - In simple linear regression :- the intercept is relatively straightforward to interpret. If the independent variable (X) can realistically take on a value of zero, the intercept (β₀) represents the mean of the dependent variable (Y) when X is zero. For instance, if a regression model predicts sales based on advertising spending, and the company sometimes spends nothing on advertising, the intercept would be the baseline sales level when there's no advertising. If X never reaches zero in the observed data, the intercept might be less meaningful, as it's outside the range of the data.\n",
        "  - In multiple Linear regression :- In multiple linear regression, the intercept is more complex to interpret because it represents the expected value of Y when all independent variables are zero, not just one. The intercept is the value of Y when all other predictors in the model are held constant at zero, according to Stats and R blog. This interpretation becomes even more nuanced when predictors are standardized or coded in specific ways. The intercept is still needed for unbiased slope estimates and accurate predictions, but its interpretation is often secondary to understanding the relationships between the predictors and the outcome, according to The Analysis Factor.\n",
        "\n",
        "**Q 16  What is the significance of the slope in regression analysis, and how does it affect predictions ?**\n",
        "  - It indicates the direction and magnitude of the relationship between the variables, impacting how accurately the model can predict the dependent variable's value based on the independent variable.\n",
        "\n",
        "**Q 17 How does the intercept in a regression model provide context for the relationship between variables?**\n",
        "  - In regression models, the intercept provides the predicted value of the dependent variable when all independent variables are zero. This baseline value acts as a reference point, helping to understand the relationship between variables by showing where the line starts when the independent variable(s) are not in effect.\n",
        "\n",
        "**Q 18 What are the limitations of using R² as a sole measure of model performance?**\n",
        "  - R-squared, while useful, has several limitations when used as the sole measure of model performance. It's sensitive to model complexity, potentially inflating R-squared with irrelevant variables. R-squared doesn't tell you whether the model is good or bad, or if the data is biased. It also doesn't measure predictive error or how well the model fits the data's structure.\n",
        "\n",
        "**Q 19 How would you interpret a large standard error for a regression coefficient?**\n",
        "  - large standard error for a regression coefficient :- the estimated coefficient is less precise and less reliable as an estimate of the true population value.\n",
        "\n",
        "**Q 20 How can heteroscedasticity be identified in residual plots, and why is it important to address it?**\n",
        "  - Heteroscedasticity in residual plots can be identified by observing a fan-shaped pattern where the spread of residuals (points on the plot) increases or decreases systematically as the fitted values (predicted values) change.\n",
        "\n",
        "**Q 21 What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?**\n",
        "  - While r-squared measures the proportion of variance in the dependent variable explained by the independent variables, it always increases when more predictors are added. Adjusted r-squared adjusts for the number of predictors and decreases if the additional variables do not contribute to the model's significance.\n",
        "\n",
        "**Q 22 Why is it important to scale variables in Multiple Linear Regression?**\n",
        "  - Scaling variables in multiple linear regression is beneficial for several reasons .\n",
        "  - Faster convergence of optimization algorithms like gradient descent.\n",
        "  - Improved interpretability of coefficients.\n",
        "  - Preventing dominant features from overshadowing others.\n",
        "\n",
        "**Q 23 What is polynomial regression?**\n",
        "  - Polynomial regression is a type of regression analysis where the relationship between a dependent variable and one or more independent variables is modeled as an nth-degree polynomial.\n",
        "\n",
        "**Q 24 How does polynomial regression differ from linear regression?**\n",
        "  - Polynomial regression differs from linear regression in the type of relationship it models between variables.\n",
        "  - Linear regression fits a straight line to the data.\n",
        "  - Polynomial regression fits a curve, allowing for more complex, non-linear relationships.\n",
        "\n",
        "**Q 25 When is polynomial regression used?**\n",
        "  - Polynomial regression is used when the relationship between variables is non-linear and can be modeled by a polynomial function, such as a quadratic, cubic, or higher-degree curve.\n",
        "\n",
        "**Q 26 What is the general equation for polynomial regression?**\n",
        "  - The general equation for polynomial regression, where y is the dependent variable and x is the independent variable, is y = β₀ + β₁x + β₂x² + ... + βₙxⁿ + ϵ\n",
        "\n",
        "**Q 27 Can polynomial regression be applied to multiple variables?**\n",
        "  - Yes, polynomial regression can be applied to multiple variables. It extends the concept of linear regression by allowing for non-linear relationships between the predictor variables and the response variable.\n",
        "\n",
        "**Q 28 What are the limitations of polynomial regression?**\n",
        "  - Polynomial regression, while powerful for modeling complex relationships, has limitations. One main issue is the risk of overfitting, especially with high-degree polynomials, where the model learns the training data too well and performs poorly on unseen data\n",
        "\n",
        "**Q 29 What methods can be used to evaluate model fit when selecting the degree of a polynomial?**\n",
        "  - Several methods can be used to evaluate model fit when selecting the degree of a polynomial, including visual inspection, cross-validation, and model comparison metrics like R-squared or adjusted R-squared\n",
        "\n",
        "**Q 30 Why is visualization important in polynomial regression?**\n",
        "  - Visualization is crucial in polynomial regression for understanding and interpreting the model's fit to the data, assessing the degree of polynomial, and detecting potential issues like overfitting or underfitting\n",
        "\n",
        "**Q 31 How is polynomial regression implemented in Python?**\n",
        "  - Polynomial regression is implemented in Python using libraries such as scikit-learn and NumPy. Here's a breakdown of the process:\n",
        "  - 1. Data Preparation:\n",
        "        The data needs to be loaded and split into independent variables (features) and dependent variables (target).\n",
        "        The independent variable's values will be transformed into polynomial terms.\n",
        "  - 2. Polynomial Transformation:\n",
        "        The PolynomialFeatures class from scikit-learn is used to generate polynomial terms from the original features.\n",
        "        The degree of the polynomial needs to be specified.\n",
        "        For example, if the degree is 2, and the original feature is x, the transformed features will be x⁰, x¹, and x².\n",
        "  - 3. Model Training:\n",
        "        Linear regression is used to fit the transformed data.\n",
        "        The LinearRegression class from scikit-learn is used.\n",
        "        The model learns the coefficients for each polynomial term.\n",
        "  - 4. Prediction:\n",
        "        The model can be used to predict new values by first transforming the new data using PolynomialFeatures and then using the predict method.\n",
        "  - 5. Visualization:\n",
        "        Libraries like Matplotlib can be used to plot the data and the fitted polynomial curve."
      ],
      "metadata": {
        "id": "zer5megUjk5L"
      }
    }
  ]
}